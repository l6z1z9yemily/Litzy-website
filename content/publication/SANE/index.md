---
title: 'SANE: Specialization-Aware Neural Network Ensemble'
publication_types:
  - "3"
authors:
  - admin
  - Kan Ren
  - Xinyang Jiang
  - Minzhe Han
  - Haipeng Zhang
  - Dongsheng Li
doi: ""
abstract: Real-world data is often generated by some complex distribution, which can be approximated by a composition of multiple simpler distributions. Thus, it is intuitive to divide the complex model learning into training several simpler models, each of which specializes in one simple distribution. Ensemble learning is one way to realize specialization, and has been widely used in practical machine learning scenarios. Many ensemble methods propose to increase diversity of base models, which could potentially result in model specialization. However, our studies show that without explicitly enforcing specification, pursuing diversity may not be enough to achieve satisfactory ensemble performance. In this paper, we propose SANE --- an end-to-end ensemble learning method that actively enforces model specification, where base models are trained to specialize in sub-regions of a latent space representing the simple distribution composition, and aggregated based on their specialties. Experiments in several prediction tasks on both image datasets and tabular datasets demonstrate the superior performance of our proposed method over state-of-the-art ensemble methods.
draft: false
tags:
  - Ensemble Learning
categories:
  - Ensemble Learning
projects: []
slides: ""
url_pdf: "https://openreview.net/pdf?id=pLNLdHrZmcX"
image:
  caption: Comparison of three different ensemble methods on the synthetic checkboard dataset (a).
 The first three columns in (b) (c) (d) are decision boundaries of two base models and the final ensemble model, resp.,
 while the last two columns indicate the samples by gray dots for which each base model makes correct predictions.
 (b) is averaging ensemble of models with randomly initialization and trained independently.
 (c) introduces a negative correlation (diversity) loss to minimize in additional to the vanilla prediction loss utilized in (b).
 (d) uses the model confidence of the ground truth labels to weight the training loss and aggregate the model predictions accordingly.
  focal_point: ""
  preview_only: false
  filename: sedge.png
summary: A novel ensemble learning approach which actively encourages model specialization and aggregates model predictions according to the estimated *sample-level model specialty*.
url_dataset: ""
url_project: ""
publication: In *arXiv*
publication_short: ""
url_source: ""
url_video: ""
featured: true
date: 2022-06-06T16:36:26.436Z
url_slides: ""
publishDate: 2017-01-01T00:00:00.000Z
url_poster: ""
url_code: ""
---
